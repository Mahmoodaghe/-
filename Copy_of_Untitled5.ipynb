{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyP7npSCELiHBc7eFz8eFE0K",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Mahmoodaghe/-/blob/main/Copy_of_Untitled5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h-S1N0Kppt_T",
        "outputId": "b62a92b3-5697-4996-816f-d2990690b289"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['This', 'is', 'an', 'example', 'sentence', 'for', 'tokenization', '.']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "# prompt: cod e to tokenize text with example\n",
        "\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "text = \"This is an example sentence for tokenization let's.\"\n",
        "tokens = word_tokenize(text)\n",
        "\n",
        "print(tokens)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: code to show the diffrent between split and  tokenizee\n",
        "\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "nltk.download('punkt')\n",
        "\n",
        "text = \"This is an example sentence for tokenization.\"\n",
        "\n",
        "# Using split()\n",
        "split_tokens = text.split()\n",
        "print(\"Tokens using split():\", split_tokens)\n",
        "\n",
        "# Using word_tokenize()\n",
        "tokens = word_tokenize(text)\n",
        "print(\"Tokens using word_tokenize():\", tokens)\n",
        "\n",
        "# Comparing the results\n",
        "print(\"\\nDifference:\")\n",
        "for i in range(len(split_tokens)):\n",
        "  if split_tokens[i] != tokens[i]:\n",
        "    print(f\"At index {i}:\")\n",
        "    print(f\"split(): {split_tokens[i]}\")\n",
        "    print(f\"word_tokenize(): {tokens[i]}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8smu5u0fqMdq",
        "outputId": "45cef54f-6f28-4ee6-9e8c-70a2f4aa5c4a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokens using split(): ['This', 'is', 'an', 'example', 'sentence', 'for', 'tokenization.']\n",
            "Tokens using word_tokenize(): ['This', 'is', 'an', 'example', 'sentence', 'for', 'tokenization', '.']\n",
            "\n",
            "Difference:\n",
            "At index 6:\n",
            "split(): tokenization.\n",
            "word_tokenize(): tokenization\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: code to print  arabic stopwords using nltk\n",
        "\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "arabic_stopwords = set(stopwords.words('arabic'))\n",
        "print(arabic_stopwords)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tlex4XiorPQP",
        "outputId": "3dbbe630-2d3c-4971-bebf-664d248fb5b3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'ثاني', 'إلى', 'تشرين', 'واهاً', 'وإن', 'اللاتي', 'سرعان', 'حيثما', 'لبيك', 'تارة', 'سابع', 'أمّا', 'إليك', 'ديسمبر', 'هذه', 'كِخ', 'خمس', 'مما', 'خ', 'حجا', 'هذا', 'حبيب', 'تاسع', 'كاد', 'صهْ', 'مئتان', 'بيد', 'فإذا', 'آض', 'كلاهما', 'دولار', 'لما', 'ثم', 'سين', 'كذلك', 'تفعلين', 'ذلكن', 'إياها', 'مكانَك', 'تي', 'كأيّ', 'مع', 'إذما', 'لاسيما', 'كأنّ', 'لمّا', 'ألف', 'هذان', 'بكم', 'ثلاثاء', 'أربعمائة', 'ذواتي', 'إحدى', 'عشرين', 'لكنما', 'اثنين', 'بسّ', 'خمسين', 'جير', 'طفق', 'مكانكما', 'إياهم', 'عين', 'أنت', 'هَذِه', 'سنتيم', 'سقى', 'ليستا', 'التي', 'لو', 'خمسمئة', 'أولئك', 'خميس', 'خال', 'أيار', 'علًّ', 'مكانكنّ', 'آ', 'كسا', 'يوان', 'طاق', 'حمٌ', 'أينما', 'إليكنّ', 'قام', 'سبت', 'ذلك', 'إن', 'عسى', 'ظاء', 'تفعلان', 'ثمانمئة', 'تعسا', 'هو', 'ضحوة', 'نون', 'تينك', 'خمسون', 'وجد', 'أجل', 'تسعمائة', 'اثنا', 'نوفمبر', 'جمعة', 'يونيو', 'الألاء', 'غدا', 'إنا', 'ذات', 'سبعمائة', 'والذين', 'ثمان', 'هَجْ', 'ثلاث', 'ولا', 'منذ', 'ئ', 'جانفي', 'ء', 'إذ', 'لدى', 'كما', 'صباح', 'فيه', 'وإذ', 'ز', 'ما أفعله', 'ءَ', 'فلان', 'أول', 'فيها', 'إلا', 'فمن', 'تَيْنِ', 'ث', 'ض', 'هيت', 'جنيه', 'لكنَّ', 'هيا', 'بضع', 'تِي', 'ثلاثون', 'واو', 'إزاء', 'تبدّل', 'لسن', 'نيف', 'ساء', 'الآن', 'أن', 'لكي', 'اللذان', 'ش', 'هبّ', 'أصلا', 'سبع', 'خلا', 'ظنَّ', 'فإن', 'أفٍّ', 'هلّا', 'صراحة', 'عليه', 'ج', 'اربعين', 'ست', 'مهما', 'اللواتي', 'ظلّ', 'هَذِي', 'هَذا', 'نبَّا', 'لهن', 'د', 'وُشْكَانَ', 'فبراير', 'ستون', 'فضلا', 'ذلكم', 'شباط', 'نَّ', 'ريث', 'صدقا', 'آناء', 'معاذ', 'إليكن', 'هؤلاء', 'وَيْ', 'عليك', 'آهاً', 'هل', 'ف', 'أوّهْ', 'أحد', 'منها', 'هذين', 'سرا', 'طالما', 'استحال', 'أيلول', 'أولاء', 'مائة', 'ذ', 'ثمّ', 'بك', 'نحو', 'فلس', 'عشر', 'ك', 'كلا', 'بات', 'لن', 'م', 'رُبَّ', 'أفعل به', 'تِه', 'دواليك', 'أنتما', 'ليت', 'ما انفك', 'إي', 'إيانا', 'أيا', 'ذال', 'حاء', 'هَذانِ', 'إياكم', 'إمّا', 'لولا', 'وما', 'بهن', 'أقل', 'وهب', 'ثمنمئة', 'ته', 'كذا', 'كرب', 'ثمة', 'غالبا', 'ثماني', 'سبتمبر', 'عوض', 'غين', 'ذي', 'قرش', 'تعلَّم', 'أخٌ', 'ليس', 'فو', 'هَاتِي', 'إيه', 'إلَيْكَ', 'كلّما', 'طَق', 'ص', 'خاء', 'آهٍ', 'أربعاء', 'أربع', 'ح', 'أيضا', 'غداة', 'ذلكما', 'تجاه', 'واحد', 'أولالك', 'إليكَ', 'مايو', 'قد', 'شيكل', 'ب', 'أنّى', 'هما', 'اخلولق', 'تفعلون', 'خمسة', 'ا', 'فيم', 'إنما', 'انبرى', 'كأين', 'سبعة', 'لسنا', 'هاتان', 'حاي', 'ستين', 'ممن', 'بعض', 'كيفما', 'تانِك', 'صبرا', 'مازال', 'حمدا', 'ذا', 'عما', 'لكيلا', 'ترك', 'حسب', 'درهم', 'عيانا', 'ستة', 'أنبأ', 'لعلَّ', 'لستن', 'ضاد', 'ستمئة', 'هذي', 'لك', 'ثلاثة', 'فرادى', 'صبر', 'هن', 'ثاء', 'كليهما', 'تسعين', 'ه', 'أيّ', 'مرّة', 'به', 'إياه', 'لكن', 'تسع', 'أي', 'نعم', 'هلم', 'إنه', 'مارس', 'ل', 'ثمانين', 'ط', 'أم', 'بها', 'صار', 'أنتن', 'كيف', 'لئن', 'عجبا', 'جيم', 'ذيت', 'ثلاثمئة', 'بلى', 'إما', 'ذين', 'أنًّ', 'لنا', 'إياهما', 'عشرون', 'س', 'أنشأ', 'ثالث', 'أنى', 'حاشا', 'بعد', 'تحوّل', 'أفريل', 'عند', 'أصبح', 'جوان', 'كي', 'يناير', 'تسعون', 'قلما', 'أربعة', 'آمينَ', 'عدَّ', 'شتانَ', 'أنا', 'الذي', 'عَدَسْ', 'هلا', 'علم', 'أربعمئة', 'مليم', 'هكذا', 'إلّا', 'لا', 'حزيران', 'تموز', 'كلَّا', 'خامس', 'ذواتا', 'ر', 'أخو', 'جميع', 'همزة', 'باء', 'آي', 'أل', 'ثمَّ', 'فلا', 'كأن', 'سبعمئة', 'أمامكَ', 'تحت', 'كم', 'غادر', 'ثامن', 'شتان', 'لا سيما', 'ارتدّ', 'ليسا', 'فيفري', 'هاك', 'منه', 'شَتَّانَ', 'تلكم', 'حبذا', 'ؤ', 'كأيّن', 'كل', 'لعل', 'ذان', 'أكتوبر', 'هم', 'نحن', 'عدا', 'أيّان', 'ثمانية', 'ليست', 'بماذا', 'أبريل', 'ذِي', 'ليسوا', 'هنالك', 'نَخْ', 'قطّ', 'و', 'لست', 'متى', 'سبعين', 'تانِ', 'تاء', 'عشرة', 'انقلب', 'اللتيا', 'هاته', 'طرا', 'كثيرا', 'اتخذ', 'نيسان', 'أبٌ', 'إنَّ', 'بخ', 'خاصة', 'أضحى', 'قاطبة', 'مه', 'كليكما', 'جعل', 'مساء', 'هَاتِه', 'بعدا', 'كأي', 'أنتِ', 'بئس', 'بما', 'الألى', 'ذينك', 'ذه', 'ظ', 'ألا', 'لهم', 'إى', 'حرى', 'إياهن', 'تلك', 'حيَّ', 'هَيْهات', 'خلف', 'سمعا', 'وا', 'آب', 'أعلم', 'عاد', 'زعم', 'بمن', 'بَلْهَ', 'حمو', 'بخٍ', 'سوف', 'إليكما', 'هللة', 'بطآن', 'أرى', 'رويدك', 'ى', 'كى', 'لعمر', 'أيها', 'آذار', 'أو', 'أجمع', 'رأى', 'الذين', 'لم', 'هَاتَيْنِ', 'ريال', 'ماي', 'صاد', 'ذانك', 'لكما', 'ها', 'ّأيّان', 'كن', 'خمسمائة', 'رجع', 'مذ', 'ق', 'هيهات', 'ين', 'هنا', 'آنفا', 'غ', 'زاي', 'صهٍ', 'كأنما', 'أوت', 'زود', 'بَسْ', 'أطعم', 'أبدا', 'ماذا', 'أوه', 'لستما', 'ابتدأ', 'قاف', 'يفعلان', 'دال', 'سبعون', 'أكثر', 'أخذ', 'هاء', 'آه', 'اثني', 'ليرة', 'وهو', 'شين', 'تين', 'ما', 'اربعون', 'إياكن', 'ذَيْنِ', 'آها', 'إذاً', 'أمام', 'عامة', 'دونك', 'هاكَ', 'حار', 'ما برح', 'دون', 'كيت', 'لستم', 'خلافا', 'أمد', 'ذوا', 'قبل', 'هاتين', 'ومن', 'كانون', 'لام', 'ت', 'راح', 'أين', 'بكن', 'خبَّر', 'هَؤلاء', 'سبحان', 'ة', 'سوى', 'مئة', 'أهلا', 'ذو', 'أُفٍّ', 'ألفى', 'والذي', 'ذانِ', 'بهما', 'مكانكم', 'أى', 'ثلاثين', 'هَذَيْنِ', 'أمس', 'يفعلون', 'هيّا', 'ذهب', 'لوما', 'على', 'ولكن', 'ميم', 'هاهنا', 'فوق', 'مادام', 'أ', 'بغتة', 'تخذ', 'فيما', 'كلما', 'لي', 'أما', 'اثنان', 'ثان', 'بهم', 'بل', 'لكم', 'ولو', 'يمين', 'بي', 'جويلية', 'إليكم', 'إياك', 'ذاك', 'أقبل', 'غير', 'حَذارِ', 'هَاتانِ', 'اللذين', 'ثمّة', 'ثلاثمائة', 'ورد', 'تلكما', 'إذا', 'لدن', 'في', 'حادي', 'شرع', 'اللتين', 'حتى', 'حدَث', 'لها', 'ن', 'شبه', 'رزق', 'كان', 'تسعة', 'حقا', 'بين', 'أسكن', 'جلل', 'مثل', 'ستمائة', 'أعطى', 'آهِ', 'إيهٍ', 'راء', 'بنا', 'وإذا', 'كاف', 'وراءَك', 'بس', 'إذن', 'نفس', 'عاشر', 'سحقا', 'يوليو', 'شمال', 'أف', 'ع', 'أبو', 'أنتم', 'يورو', 'كلتا', 'فاء', 'أمامك', 'يا', 'ذِه', 'مافتئ', 'عن', 'تلقاء', 'ي', 'من', 'تسعمئة', 'ياء', 'إياي', 'إياكما', 'بكما', 'أمسى', 'بؤسا', 'لهما', 'عل', 'علق', 'سادس', 'حين', 'دينار', 'نا', 'درى', 'أوشك', 'اللائي', 'له', 'أخبر', 'اللتان', 'لات', 'أغسطس', 'حيث', 'ثمانون', 'رابع', 'هناك', 'هي', 'هاتي', 'طاء'}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: code to print  stopwords using nltk\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "\n",
        "english_stopwords = set(stopwords.words('english'))\n",
        "print(english_stopwords)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VNAuhKI9r7lw",
        "outputId": "ab396f7b-ee4f-4e06-fe59-585e71292baf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'was', 'having', 'haven', 'because', \"mightn't\", 'she', 'after', 'wasn', \"she's\", 'her', 'at', 'too', 'for', 'to', 'hasn', \"weren't\", \"wouldn't\", 'during', 'needn', 'by', 'your', 'off', 'shan', \"you'll\", 'as', 'ma', 'won', 'but', 'all', 'here', 'few', 'out', 'those', 'be', 'through', 'from', 'own', 'being', 'now', 'above', 'each', 'is', 'he', 'we', 'myself', 'than', \"you've\", 'i', 'how', 'no', 'aren', 'doesn', \"hadn't\", 'its', \"didn't\", 'why', 'yourselves', 'should', 'up', 'don', \"it's\", 'yourself', 'that', 'in', 'down', 'or', 'ain', 'then', \"aren't\", 'only', 'most', 'theirs', 'when', 'with', 'same', 'couldn', 'into', 'under', 'can', 'this', 'some', 'other', 'y', 'mightn', 'just', 'so', 'our', 'wouldn', 'did', \"hasn't\", 'before', 'more', 'of', 'they', \"shouldn't\", 'nor', 'himself', \"haven't\", 'below', \"mustn't\", 'you', 'who', 're', \"won't\", 'have', 'am', \"couldn't\", 'hadn', 'it', 'were', 'been', 'between', 'an', 'does', 'll', \"wasn't\", 'whom', \"you're\", 'until', 'me', 'are', 'them', 'has', \"needn't\", 'any', 'on', 'against', 's', 'while', 'o', 'ours', 'where', 'itself', 'what', 'weren', 'themselves', \"doesn't\", 'doing', 'over', \"should've\", 'him', 'hers', 'my', 'd', 'if', 'had', 'a', 'there', 've', 'isn', 'once', 'about', 'the', 'herself', 'very', 'ourselves', 'both', \"shan't\", 'mustn', 'such', \"isn't\", 'his', 'these', 'do', 'their', 'shouldn', 'which', 'not', \"don't\", 'didn', 'further', 'will', 'm', \"that'll\", 'again', \"you'd\", 'yours', 'and', 't'}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: code for the number of stopnumber in arabic\n",
        "\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "arabic_stopwords = set(stopwords.words('arabic'))\n",
        "print(\"Number of Arabic stop words:\", len(arabic_stopwords))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "81DKB_aDspUZ",
        "outputId": "3b5d421b-532f-4e2c-9207-c5a2b686ac76"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of Arabic stop words: 701\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: code for count stop word in nltK\n",
        "\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "text = \"This is an example sentence with some stop words.\"\n",
        "tokens = word_tokenize(text)\n",
        "\n",
        "english_stopwords = set(stopwords.words('english'))\n",
        "\n",
        "stop_word_count = 0\n",
        "for token in tokens:\n",
        "  if token.lower() in english_stopwords:\n",
        "    stop_word_count += 1\n",
        "\n",
        "print(\"Number of stop words:\", stop_word_count)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fvmCR8YntElq",
        "outputId": "3f1495f7-4106-453e-d63e-9675b0655e87"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of stop words: 5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: code to make  my stopwords\n",
        "\n",
        "my_stopwords = [\"this\", \"is\", \"a\", \"an\", \"the\", \"of\", \"to\", \"in\", \"on\", \"at\", \"by\", \"for\", \"with\", \"about\", \"as\", \"from\", \"into\", \"like\", \"through\", \"after\", \"over\", \"between\", \"out\", \"against\", \"during\", \"without\", \"before\", \"under\", \"around\", \"among\"]\n"
      ],
      "metadata": {
        "id": "mH2e_R8Ft4wF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: code to make  my arabic stopwords\n",
        "\n",
        "my_arabic_stopwords = [\"من\", \"إلى\", \"في\", \"على\", \"عن\", \"مع\", \"و\", \"أن\", \"هو\", \"هي\", \"هما\", \"هم\", \"هن\", \"كان\", \"كانت\", \"كنا\", \"كنتم\", \"كنتن\", \"يكون\", \"تكون\", \"نكون\", \"يكونون\", \"يكونن\", \"يكونوا\", \"أنه\", \"إنه\", \"إنها\", \"إنهم\", \"إنكن\", \"إنهم\", \"إنّهن\", \"ذلك\", \"هذه\", \"هؤلاء\", \"أولئك\", \"الذي\", \"التي\", \"الذين\", \"لاتي\", \"اللتان\", \"اللتيا\", \"اللذان\", \"اللذين\", \"اللاتي\", \"اللائي\", \"اللتيا\", \"اللتان\", \"اللتين\", \"اللذين\", \"اللذان\", \"اللذين\", \"اللاتي\", \"اللائي\", \"اللذان\", \"اللذين\", \"اللاتي\", \"اللائي\", \"هو\", \"هي\", \"هما\", \"هم\", \"هن\", \"أنت\", \"أنتما\", \"أنتم\", \"أنتن\", \"أنا\", \"نحن\", \"إنه\", \"إنها\", \"إنهم\", \"إنكن\", \"إنهن\", \"إنهم\", \"إنّهن\", \"ماذا\", \"متى\", \"أين\", \"كيف\", \"لماذا\", \"لكن\", \"أو\", \"إلا\", \"حتى\", \"ثم\", \"بعد\", \"قبل\", \"بين\", \"فوق\", \"تحت\", \"خلف\", \"أمام\", \"حول\", \"جميع\", \"كل\", \"بعض\", \"لا\", \"ليس\", \"لم\", \"لن\", \"ما\", \"منذ\", \"إلى\", \"عند\", \"مع\", \"بدون\", \"دون\", \"حسب\", \"مثل\", \"فقط\", \"أي\", \"أيضا\", \"أيضاً\", \"إن\", \"أن\", \"لأن\", \"إذا\", \"ف\", \"ثم\", \"ولكن\", \"مع\", \"من\", \"في\", \"على\", \"عن\", \"إلى\", \"حيث\", \"الذي\", \"التي\", \"الذين\", \"التي\", \"و\", \"ب\", \"ك\", \"ل\", \"أ\", \"ت\"]\n",
        "\n",
        "print(my_arabic_stopwords)\n"
      ],
      "metadata": {
        "id": "B_A_FIiOuP3E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: code for count my stopwords\n",
        "\n",
        "text = \"This is an example sentence with some stop words.\"\n",
        "tokens = word_tokenize(text)\n",
        "\n",
        "stop_word_count = 0\n",
        "for token in tokens:\n",
        "  if token.lower() in my_stopwords:\n",
        "    stop_word_count += 1\n",
        "\n",
        "print(\"Number of stop words (using my_stopwords):\", stop_word_count)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "DxAuW1V8ugl-",
        "outputId": "26849b84-d5c9-4663-b2c8-a9f59431b944"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'my_stopwords' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-bb2d103a86cb>\u001b[0m in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mstop_word_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtokens\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m   \u001b[0;32mif\u001b[0m \u001b[0mtoken\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmy_stopwords\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0mstop_word_count\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'my_stopwords' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: code to remove stopwords sing if\n",
        "\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "text = \"This is an example sentence with some stop words.\"\n",
        "tokens = word_tokenize(text)\n",
        "\n",
        "english_stopwords = set(stopwords.words('english'))\n",
        "\n",
        "filtered_tokens = []\n",
        "for token in tokens:\n",
        "    if token.lower() not in english_stopwords:\n",
        "        filtered_tokens.append(token)\n",
        "\n",
        "print(\"Tokens without stop words:\", filtered_tokens)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uB23dOMFvZMC",
        "outputId": "9f44597c-e01f-4147-a16e-f9b3dcb7e429"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokens without stop words: ['example', 'sentence', 'stop', 'words', '.']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: code to remove stopwords using\n",
        "\n",
        "nltk.download('stopwords')\n",
        "\n",
        "text = \"This is an example sentence with some stop words.\"\n",
        "tokens = word_tokenize(text)\n",
        "\n",
        "english_stopwords = set(stopwords.words('english'))\n",
        "\n",
        "filtered_tokens = [token for token in tokens if token.lower() not in english_stopwords]\n",
        "\n",
        "print(\"Tokens without stop words:\", filtered_tokens)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UcT4TW8DwOut",
        "outputId": "e71b21c6-db66-465d-e470-728137ff5532"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokens without stop words: ['example', 'sentence', 'stop', 'words', '.']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "للتحويل الى بادىه والاحقة"
      ],
      "metadata": {
        "id": "3RDYoEEwyhjd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: code to show the diffrent between stemming and lemmatization using nltk\n",
        "\n",
        "import nltk\n",
        "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
        "from nltk.corpus import wordnet\n",
        "\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "\n",
        "stemmer = PorterStemmer()\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "words = [\"running\", \"ran\", \"runs\", \"easily\", \"better\", \"best\"]\n",
        "\n",
        "print(\"Stemming:\")\n",
        "for word in words:\n",
        "  print(f\"{word}: {stemmer.stem(word)}\")\n",
        "\n",
        "\n",
        "print(\"\\nLemmatization:\")\n",
        "for word in words:\n",
        "  print(f\"{word}: {lemmatizer.lemmatize(word, pos=wordnet.VERB)}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CjNCLqqDwgGI",
        "outputId": "f8077c07-6519-443f-e82a-bc9aaf315c6e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Stemming:\n",
            "running: run\n",
            "ran: ran\n",
            "runs: run\n",
            "easily: easili\n",
            "better: better\n",
            "best: best\n",
            "\n",
            "Lemmatization:\n",
            "running: run\n",
            "ran: run\n",
            "runs: run\n",
            "easily: easily\n",
            "better: better\n",
            "best: best\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "للتحويل الى المصدر"
      ],
      "metadata": {
        "id": "ZQZ9gVwlyYU1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: code to show the diffrent between stemming and lemmatization using spacy\n",
        "\n",
        "!pip install spacy\n",
        "!python -m spacy download en_core_web_sm\n",
        "\n",
        "import spacy\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "def show_stem_lemma(text):\n",
        "  doc = nlp(text)\n",
        "  print(\"Word\\tStem\\tLemma\")\n",
        "  for token in doc:\n",
        "    print(f\"{token.text}\\t{token.lemma_}\\t{token.morph.get('Inflection')}\")\n",
        "\n",
        "text = \"running runs easily better best\"\n",
        "show_stem_lemma(text)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "56WtEcKQxTC4",
        "outputId": "69624474-a636-4771-a9da-2d8bbe3e5791"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: spacy in /usr/local/lib/python3.10/dist-packages (3.7.6)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (8.2.5)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (0.12.5)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (4.66.5)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.32.3)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.9.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.1.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy) (71.0.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (24.1)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.4.0)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.26.4)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.10/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.2.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.23.4)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2024.8.30)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy) (0.1.5)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (8.1.7)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (13.8.1)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.19.0)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (7.0.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy) (2.1.5)\n",
            "Requirement already satisfied: marisa-trie>=0.7.7 in /usr/local/lib/python3.10/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.2.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.18.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy) (1.16.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.2)\n",
            "Collecting en-core-web-sm==3.7.1\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.7.1/en_core_web_sm-3.7.1-py3-none-any.whl (12.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m44.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy<3.8.0,>=3.7.2 in /usr/local/lib/python3.10/dist-packages (from en-core-web-sm==3.7.1) (3.7.6)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.2.5)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.12.5)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.66.5)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.32.3)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.9.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.1.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (71.0.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (24.1)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.4.0)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.26.4)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.10/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.2.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.23.4)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2024.8.30)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.1.5)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.1.7)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (13.8.1)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.19.0)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (7.0.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.1.5)\n",
            "Requirement already satisfied: marisa-trie>=0.7.7 in /usr/local/lib/python3.10/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.2.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.18.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.16.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.1.2)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n",
            "Word\tStem\tLemma\n",
            "running\trun\t[]\n",
            "runs\trun\t[]\n",
            "easily\teasily\t[]\n",
            "better\twell\t[]\n",
            "best\twell\t[]\n"
          ]
        }
      ]
    }
  ]
}